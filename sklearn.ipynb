{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From ffnn.ipynb\n",
    "def countSSError(results, expected_results):\n",
    "    results = np.array(results)\n",
    "    expected_results = np.array(expected_results)\n",
    "    if results.shape != expected_results.shape:\n",
    "        print(\"Error: array shape mismatch\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return np.sum(np.square(results - expected_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn Using Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKLearn MLPClassifier with 5,3,3,2 hidden layers and relu activation function\n",
      "Prediction:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "\n",
      "Target:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "\n",
      "Jumlah layer: 5\n",
      "Jumlah iterasi: 1000\n",
      "Jumlah fitur: 4\n",
      "Jumlah output: 3\n",
      "Weight Input: \n",
      " [[-2.11237097e-02  3.13107969e-01 -1.55344599e-01 -4.62181600e-01\n",
      "   2.39886078e-01]\n",
      " [-2.71512518e-01 -8.26239061e-01 -1.39626167e-04 -3.79599188e-01\n",
      "   3.28595945e-01]\n",
      " [-2.10151629e-02  7.62541414e-01 -1.82147958e-02  7.46259078e-01\n",
      "  -4.36163580e-01]\n",
      " [-1.01932685e-03  3.41339274e-01  2.18626578e-11 -2.59251850e-01\n",
      "  -4.49383250e-01]]\n",
      "Bias Input: \n",
      " [ 0.4703414   0.53332317 -0.30467704 -0.03067309  1.04797845]\n",
      "Weight Hidden Layer: \n",
      " [[ 8.51708765e-02 -1.01940849e-01 -1.44894690e-01]\n",
      " [-8.10160230e-01  9.96478234e-01 -9.08639096e-02]\n",
      " [-3.48804032e-08  1.41863424e-01  2.27344797e-17]\n",
      " [ 1.63872312e-03 -7.69928178e-01  1.30997930e-03]\n",
      " [ 4.04891959e-02 -8.50641474e-01  9.95995989e-03]]\n",
      "Bias Hidden Layer: \n",
      " [ 0.58431878  0.16401628 -0.38028216]\n",
      "Weight Output: \n",
      " [[ 4.12289684e-01 -1.42125930e-01 -1.62174677e-10]\n",
      " [ 1.09177338e+00 -7.42305832e-03 -8.80816135e-03]\n",
      " [-1.12861095e-01 -2.48822898e-01  2.83993894e-03]]\n",
      "Bias Output: \n",
      " [-0.82884005 -0.46890668 -0.01685368]\n",
      "Error: 32.666666666666664 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "learning_data = np.array(iris.data)\n",
    "learning_target = np.array(iris.target)\n",
    "\n",
    "classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(5,3,3), \n",
    "    activation='relu', \n",
    "    max_iter=1000, \n",
    "    learning_rate='constant', \n",
    "    learning_rate_init=0.001, \n",
    "    random_state=1\n",
    "    )\n",
    "\n",
    "classifier.fit(learning_data, learning_target)\n",
    "y_pred = classifier.predict(learning_data)\n",
    "\n",
    "print(\"SKLearn MLPClassifier with 5,3,3,2 hidden layers and relu activation function\")\n",
    "print(f\"Prediction:\\n{y_pred}\")\n",
    "print()\n",
    "print(f\"Target:\\n{learning_target}\")\n",
    "print()\n",
    "\n",
    "weight_data = classifier.coefs_\n",
    "bias_data = classifier.intercepts_\n",
    "\n",
    "print(\"layer count: {}\".format(classifier.n_layers_))\n",
    "print(\"iteration count: {}\".format(classifier.n_iter_))\n",
    "print(\"output count: {}\".format(classifier.n_outputs_))\n",
    "\n",
    "print(\"Weight Input: \\n\", weight_data[0])\n",
    "print(\"Bias Input: \\n\", bias_data[0])\n",
    "print(\"Weight Output: \\n\", weight_data[2])\n",
    "print(\"Bias Output: \\n\", bias_data[2])\n",
    "\n",
    "error = countSSError(y_pred, learning_target)\n",
    "print(f\"Error: {error / len(learning_data) * 100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow Using Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(y_true, y_pred, activation_function):\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "    loss = tf.reduce_sum(squared_error) / tf.cast(tf.shape(y_true)[0], tf.float32)\n",
    "    return 0.5 * loss\n",
    "\n",
    "# Load the JSON data\n",
    "with open('testcase/mlp.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Weights:\n",
      "Layer 1 weights:\n",
      "[[ 0.08592001  0.32276   ]\n",
      " [-0.33872002  0.46172   ]\n",
      " [ 0.449984    0.440072  ]]\n",
      "Layer 2 weights:\n",
      "[[ 0.2748    0.188   ]\n",
      " [ 0.435904 -0.53168 ]\n",
      " [ 0.68504   0.7824  ]]\n",
      "Stopped by: max_iteration\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "class TensorModel(tf.keras.Model):\n",
    "    def __init__(self, input_size, layers):\n",
    "        super(TensorModel, self).__init__()\n",
    "        self.dense_layers = []\n",
    "        for i, layer in enumerate(layers):\n",
    "            activation_function = layer[\"activation_function\"]\n",
    "            num_neurons = layer[\"number_of_neurons\"]\n",
    "            self.dense_layers.append(tf.keras.layers.Dense(num_neurons, activation=activation_function, use_bias=False, name=f'dense_{i}'))\n",
    "        self(tf.keras.Input(shape=(input_size,)))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.dense_layers:\n",
    "            # Concatenate a column vector of ones to the input data\n",
    "            x_with_bias = tf.concat([tf.ones((tf.shape(x)[0], 1)), x], axis=1)\n",
    "            x = layer(x_with_bias)\n",
    "        return x\n",
    "\n",
    "# Parse JSON data\n",
    "input_size = data[\"case\"][\"model\"][\"input_size\"] \n",
    "layers = data[\"case\"][\"model\"][\"layers\"]\n",
    "\n",
    "# Extract input data from JSON and add bias\n",
    "json_input = data[\"case\"][\"input\"]\n",
    "input_with_bias = json_input\n",
    "input_data = tf.constant(input_with_bias, dtype=tf.float32)\n",
    "\n",
    "# Extract target data from JSON and reshape it\n",
    "json_target = data[\"case\"][\"target\"]\n",
    "target_data = tf.constant(json_target, dtype=tf.float32)\n",
    "height, width = target_data.shape\n",
    "target_data = tf.reshape(target_data, [1, height, width])\n",
    "\n",
    "# Extract other parameters from JSON\n",
    "initial_weights = [tf.constant(weights, dtype=tf.float32) for weights in data[\"case\"][\"initial_weights\"]]\n",
    "learning_rate = data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n",
    "batch_size = data[\"case\"][\"learning_parameters\"][\"batch_size\"]\n",
    "max_iteration = data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n",
    "error_threshold = data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n",
    "\n",
    "# Initialize the model\n",
    "model = TensorModel(input_size, layers)\n",
    "\n",
    "# Set the initial weights for the dense layers\n",
    "for i, weights in enumerate(initial_weights):\n",
    "    model.dense_layers[i].set_weights([weights])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "# Training\n",
    "for iteration in range(max_iteration):\n",
    "    total_loss = 0\n",
    "    for i in range(0, input_data.shape[0], batch_size):\n",
    "        x_batch = input_data[i:i+batch_size]\n",
    "        y_batch = target_data[i:i+batch_size]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(x_batch, training=True)\n",
    "            # activation_function = layers[i][\"activation_function\"]\n",
    "            loss = loss_function(y_batch, predictions, activation_function)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        total_loss += loss\n",
    "        \n",
    "    avg_loss = total_loss / (input_data.shape[0] / batch_size) \n",
    "\n",
    "    # Check if the average loss is below the error threshold\n",
    "    if avg_loss < error_threshold:\n",
    "        print(f\"Error threshold reached. Stopping training at iteration {iteration + 1}.\")\n",
    "        break\n",
    "\n",
    "# Print the final weights and check\n",
    "final_weights = [layer.get_weights()[0] for layer in model.dense_layers]\n",
    "expected_final_weights = data[\"expect\"][\"final_weights\"]\n",
    "\n",
    "print(\"Final Weights:\")\n",
    "for i, weights in enumerate(final_weights):\n",
    "    print(f\"Layer {i+1} weights:\")\n",
    "    print(weights)\n",
    "print(\"Stopped by:\", \"max_iteration\" if iteration == max_iteration - 1 else \"error_threshold\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
