{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From ffnn.ipynb\n",
    "def countSSError(results, expected_results):\n",
    "    results = np.array(results)\n",
    "    expected_results = np.array(expected_results)\n",
    "    if results.shape != expected_results.shape:\n",
    "        print(\"Error: array shape mismatch\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return np.sum(np.square(results - expected_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn Using Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKLearn MLPClassifier with 5,3,3,2 hidden layers and relu activation function\n",
      "Prediction:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "\n",
      "Target:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "\n",
      "Jumlah layer: 5\n",
      "Jumlah iterasi: 1000\n",
      "Jumlah fitur: 4\n",
      "Jumlah output: 3\n",
      "Weight Input: \n",
      " [[-2.11237097e-02  3.13107969e-01 -1.55344599e-01 -4.62181600e-01\n",
      "   2.39886078e-01]\n",
      " [-2.71512518e-01 -8.26239061e-01 -1.39626167e-04 -3.79599188e-01\n",
      "   3.28595945e-01]\n",
      " [-2.10151629e-02  7.62541414e-01 -1.82147958e-02  7.46259078e-01\n",
      "  -4.36163580e-01]\n",
      " [-1.01932685e-03  3.41339274e-01  2.18626578e-11 -2.59251850e-01\n",
      "  -4.49383250e-01]]\n",
      "Bias Input: \n",
      " [ 0.4703414   0.53332317 -0.30467704 -0.03067309  1.04797845]\n",
      "Weight Hidden Layer: \n",
      " [[ 8.51708765e-02 -1.01940849e-01 -1.44894690e-01]\n",
      " [-8.10160230e-01  9.96478234e-01 -9.08639096e-02]\n",
      " [-3.48804032e-08  1.41863424e-01  2.27344797e-17]\n",
      " [ 1.63872312e-03 -7.69928178e-01  1.30997930e-03]\n",
      " [ 4.04891959e-02 -8.50641474e-01  9.95995989e-03]]\n",
      "Bias Hidden Layer: \n",
      " [ 0.58431878  0.16401628 -0.38028216]\n",
      "Weight Output: \n",
      " [[ 4.12289684e-01 -1.42125930e-01 -1.62174677e-10]\n",
      " [ 1.09177338e+00 -7.42305832e-03 -8.80816135e-03]\n",
      " [-1.12861095e-01 -2.48822898e-01  2.83993894e-03]]\n",
      "Bias Output: \n",
      " [-0.82884005 -0.46890668 -0.01685368]\n",
      "Error: 32.666666666666664 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "learning_data = np.array(iris.data)\n",
    "learning_target = np.array(iris.target)\n",
    "\n",
    "classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(5,3,3), \n",
    "    activation='relu', \n",
    "    max_iter=1000, \n",
    "    learning_rate='constant', \n",
    "    learning_rate_init=0.001, \n",
    "    random_state=1\n",
    "    )\n",
    "\n",
    "classifier.fit(learning_data, learning_target)\n",
    "y_pred = classifier.predict(learning_data)\n",
    "\n",
    "print(\"SKLearn MLPClassifier with 5,3,3,2 hidden layers and relu activation function\")\n",
    "print(f\"Prediction:\\n{y_pred}\")\n",
    "print()\n",
    "print(f\"Target:\\n{learning_target}\")\n",
    "print()\n",
    "\n",
    "weight_data = classifier.coefs_\n",
    "bias_data = classifier.intercepts_\n",
    "\n",
    "print(\"layer count: {}\".format(classifier.n_layers_))\n",
    "print(\"iteration count: {}\".format(classifier.n_iter_))\n",
    "print(\"output count: {}\".format(classifier.n_outputs_))\n",
    "\n",
    "print(\"Weight Input: \\n\", weight_data[0])\n",
    "print(\"Bias Input: \\n\", bias_data[0])\n",
    "print(\"Weight Output: \\n\", weight_data[2])\n",
    "print(\"Bias Output: \\n\", bias_data[2])\n",
    "\n",
    "error = countSSError(y_pred, learning_target)\n",
    "print(f\"Error: {error / len(learning_data) * 100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn Using Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(file_path):\n",
    "    data = load_json(file_path)\n",
    "\n",
    "    # Extract model parameters\n",
    "    input_size = data[\"case\"][\"model\"][\"input_size\"]\n",
    "    layers = data[\"case\"][\"model\"][\"layers\"]\n",
    "    initial_weights = np.array(data[\"case\"][\"initial_weights\"][0])  # Extracting weights from nested list\n",
    "    target_output = np.array(data[\"case\"][\"target\"])\n",
    "    learning_rate = data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n",
    "    batch_size = data[\"case\"][\"learning_parameters\"][\"batch_size\"]\n",
    "    max_iteration = data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n",
    "    error_threshold = data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n",
    "    stopped_by = \"max_iteration\"\n",
    "\n",
    "    # Initialize MLPRegressor model\n",
    "    model = MLPRegressor(hidden_layer_sizes=[layer[\"number_of_neurons\"] for layer in layers],\n",
    "                         activation='identity',  \n",
    "                         solver='adam',  \n",
    "                         batch_size=batch_size,\n",
    "                         learning_rate_init=learning_rate,\n",
    "                         max_iter=max_iteration, \n",
    "                         random_state=1)\n",
    "\n",
    "    print(\"Input Size:\", input_size)\n",
    "    print(\"Layers:\", layers)\n",
    "    print(\"Initial Weights:\\n\", initial_weights)\n",
    "    print(\"Target Output:\\n\", target_output)\n",
    "    print(\"Learning Rate:\", learning_rate)\n",
    "    print(\"Batch Size:\", batch_size)\n",
    "    print(\"Max Iteration:\", max_iteration)\n",
    "    print(\"Error Threshold:\", error_threshold)\n",
    "    print(\"\\n------------------------------------\\n\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(data[\"case\"][\"input\"], target_output)\n",
    "    predicted_output = model.predict(data[\"case\"][\"input\"])\n",
    "    error = mean_squared_error(target_output, predicted_output)\n",
    "\n",
    "    # Check stopping criterion\n",
    "    if error <= error_threshold:\n",
    "        stopped_by = \"error_threshold\"\n",
    "\n",
    "    print(data[\"case\"][\"input\"])\n",
    "    print(f\"\\n{target_output}\")\n",
    "    print(f\"\\n{predicted_output}\")\n",
    "    print(\"\\n-------------------------------\\n\")\n",
    "\n",
    "    # Get final weights\n",
    "    final_weights = model.coefs_\n",
    "\n",
    "    # Compare final weights with expected final weights\n",
    "    expected_final_weights = np.array(data[\"expect\"][\"final_weights\"][0])\n",
    "\n",
    "    # Output stopping criterion and final weights\n",
    "    print(\"Stopped by:\", stopped_by)\n",
    "    print(\"Final weights:\")\n",
    "    print(final_weights[1])\n",
    "    print(\"Expected Weights\")\n",
    "    print(expected_final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size: 2\n",
      "Layers: [{'number_of_neurons': 3, 'activation_function': 'linear'}]\n",
      "Initial Weights:\n",
      " [[ 0.1  0.3  0.2]\n",
      " [ 0.4  0.2 -0.7]\n",
      " [ 0.1 -0.8  0.5]]\n",
      "Target Output:\n",
      " [[ 2.   0.3 -1.9]\n",
      " [ 1.3 -0.7  0.1]]\n",
      "Learning Rate: 0.1\n",
      "Batch Size: 2\n",
      "Max Iteration: 1\n",
      "Error Threshold: 0.0\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "[[3.0, 1.0], [1.0, 2.0]]\n",
      "\n",
      "[[ 2.   0.3 -1.9]\n",
      " [ 1.3 -0.7  0.1]]\n",
      "\n",
      "[[-1.39816154 -0.42274121 -1.22749194]\n",
      " [ 0.02255002 -1.57869784  0.43272905]]\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Stopped by: max_iteration\n",
      "Final weights:\n",
      "[[-0.02236653 -0.06161119  0.47043898]\n",
      " [-0.69109547  0.65623505 -0.84522489]\n",
      " [ 0.24093502 -0.06539046  0.21737965]]\n",
      "Expected Weights\n",
      "[[ 0.22  0.36  0.11]\n",
      " [ 0.64  0.3  -0.89]\n",
      " [ 0.28 -0.7   0.37]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "file_path = \"testcase/linear.json\"\n",
    "train_model(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Weights:\n",
      "[[0.23291178 0.06015345]\n",
      " [0.12884086 0.6484948 ]\n",
      " [0.837615   0.23158205]]\n",
      "Stopped by: max_iteration\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the JSON data\n",
    "with open('testcase/sigmoid.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the model architecture\n",
    "class TensorModel(tf.keras.Model):\n",
    "    def __init__(self, input_size, num_neurons, activation_function):\n",
    "        super(TensorModel, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(num_neurons, activation=activation_function, use_bias=False)\n",
    "        self(tf.keras.Input(shape=(input_size + 1,)))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(y_true, y_pred, activation_function):\n",
    "    if activation_function == \"softmax\":\n",
    "        loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "        return loss_func(y_true, y_pred)\n",
    "    else:\n",
    "        squared_error = tf.square(y_true - y_pred)\n",
    "        loss = tf.reduce_sum(squared_error) / tf.cast(tf.shape(y_true)[0], tf.float32)\n",
    "        return 0.5 * loss\n",
    "\n",
    "# Parse JSON data\n",
    "input_size = data[\"case\"][\"model\"][\"input_size\"]\n",
    "num_neurons = data[\"case\"][\"model\"][\"layers\"][0][\"number_of_neurons\"]\n",
    "\n",
    "# Extract input data from JSON and add bias\n",
    "json_input = data[\"case\"][\"input\"]\n",
    "input_with_bias = [[1.0] + input_row for input_row in json_input]\n",
    "input_data = tf.constant(input_with_bias, dtype=tf.float32)\n",
    "\n",
    "# Extract target data from JSON and reshape it\n",
    "json_target = data[\"case\"][\"target\"]\n",
    "target_data = tf.constant(json_target, dtype=tf.float32)\n",
    "height, width = target_data.shape\n",
    "target_data = tf.reshape(target_data, [1, height, width])\n",
    "\n",
    "initial_weights = [tf.constant(weights, dtype=tf.float32) for weights in data[\"case\"][\"initial_weights\"]]\n",
    "learning_rate = data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n",
    "batch_size = data[\"case\"][\"learning_parameters\"][\"batch_size\"]\n",
    "max_iteration = data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n",
    "error_threshold = data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n",
    "activation_function = data[\"case\"][\"model\"][\"layers\"][0][\"activation_function\"]\n",
    "\n",
    "# Print the extracted data\n",
    "# print(\"initial weights:\", initial_weights)\n",
    "# print(\"input data:\", input_data)\n",
    "# print(\"target data:\", target_data)\n",
    "\n",
    "# Initialize the model\n",
    "model = TensorModel(input_size, num_neurons, activation_function)\n",
    "\n",
    "# Set the initial weights for the dense layer\n",
    "model.set_weights(initial_weights)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "# Training\n",
    "for iteration in range(max_iteration):\n",
    "    total_loss = 0\n",
    "    for i in range(0, input_data.shape[0], batch_size):\n",
    "        x_batch = input_data[i:i+batch_size]\n",
    "        y_batch = target_data[i:i+batch_size]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(x_batch, training=True)\n",
    "            loss = loss_function(y_batch, predictions, activation_function)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        total_loss += loss\n",
    "        \n",
    "    avg_loss = total_loss / (input_data.shape[0] / batch_size) \n",
    "\n",
    "    # Check if the average loss is below the error threshold\n",
    "    if avg_loss < error_threshold:\n",
    "        print(f\"Error threshold reached. Stopping training at iteration {iteration + 1}.\")\n",
    "        break\n",
    "\n",
    "# Print the final weights and check\n",
    "final_weights = model.dense.get_weights()[0]\n",
    "expected_final_weights = data[\"expect\"][\"final_weights\"]\n",
    "\n",
    "print(\"Final Weights:\")\n",
    "print(final_weights)\n",
    "print(\"Stopped by:\", \"max_iteration\" if iteration == max_iteration - 1 else \"error_threshold\")\n",
    "print(\"-------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
